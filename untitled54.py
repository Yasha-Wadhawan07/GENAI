# -*- coding: utf-8 -*-
"""Untitled54.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4vsURFzNaYXjtxuAFM0gb2zSJD6wBuz
"""

# -*- coding: utf-8 -*-
"""
A simple Recurrent Neural Network (RNN) implementation using Keras's SimpleRNN
layer to predict the next value in a synthetic time series sequence.
"""

# --- 1. Imports ---
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# --- 2. Synthetic Data Generation ---

# Generate a sequence based on a simple function (e.g., sine wave with noise)
TIME_STEPS = 400
np.random.seed(42)
# FIX: Corrected typo from TIME_STEES to TIME_STEPS
time = np.arange(0, TIME_STEPS, 1)
# Create sequential data with a pattern and some noise
raw_data = np.sin(time * 0.1) + (time / 100.0) + np.random.normal(0, 0.1, TIME_STEPS)
raw_data = raw_data.reshape(-1, 1) # Reshape to (N samples, 1 feature)

# --- 3. Preprocessing (Scaling and Sequence Creation) ---

# Import the necessary scaler
from sklearn.preprocessing import MinMaxScaler

# Scale the data to be between 0 and 1 (essential for RNN stability)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(raw_data)

# Function to create sequences (X: input sequence, Y: value to predict)
def create_sequences(dataset, look_back=10):
    X, Y = [], []
    for i in range(len(dataset) - look_back):
        # X: The sequence of 'look_back' previous values
        seq = dataset[i:(i + look_back), 0]
        X.append(seq)
        # Y: The next value in the sequence
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

# Define the look-back window (sequence length the RNN sees)
LOOK_BACK = 10

# Create the sequences
X, Y = create_sequences(scaled_data, LOOK_BACK)

# Reshape input to be (samples, time_steps, features) - Keras RNN requirement
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Split into training and testing sets (e.g., 80% train, 20% test)
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
Y_train, Y_test = Y[:train_size], Y[train_size:]

print(f"RNN Input Sequence Length (Look Back): {LOOK_BACK}")
print(f"Training data shape: X={X_train.shape}, Y={Y_train.shape}")


# --- 4. Define and Train SimpleRNN Model ---

# Define the SimpleRNN model architecture
model = Sequential([
    # SimpleRNN layer: The core recurrent layer.
    # units=32 defines the dimensionality of the hidden state (memory)
    SimpleRNN(units=32, input_shape=(LOOK_BACK, 1), activation='relu'),
    # Output dense layer: Predicts a single continuous value
    Dense(1)
], name='Simple_RNN_Model')

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

print("\n--- Model Summary ---")
model.summary()

# Train the model
print("\nTraining the Simple RNN...")
history = model.fit(
    X_train, Y_train,
    epochs=10,                # Keep epochs low for simplicity/speed
    batch_size=16,
    validation_data=(X_test, Y_test),
    verbose=0
)

print("\nTraining finished.")


# --- 5. Evaluate and Visualize Results ---

# Make predictions on the test set
predicted_scaled = model.predict(X_test)

# Inverse transform predictions to original scale for visualization
predicted_actual = scaler.inverse_transform(predicted_scaled)
Y_test_actual = scaler.inverse_transform(Y_test.reshape(-1, 1))

# Create indices for the test set predictions on the original timeline
test_indices = np.arange(train_size + LOOK_BACK, len(raw_data))

# Plot the comparison
plt.figure(figsize=(15, 6))

# Plot the full original data
plt.plot(raw_data, label='Full Original Data', color='gray', alpha=0.6)

# Plot the predictions over the test set range
plt.plot(test_indices, predicted_actual, label='RNN Predictions', color='blue', linewidth=2)
plt.plot(test_indices, Y_test_actual, label='Actual Test Values', color='red', linestyle=':')

plt.title('Simple RNN Prediction of Sequential Data')
plt.xlabel('Time Step')
plt.ylabel('Value')
# Highlight the split point between training and testing data
plt.axvline(x=train_size + LOOK_BACK, color='green', linestyle='--', linewidth=1, label='Train/Test Split Start')
plt.legend()
plt.grid(True)
plt.show()

